==============================================
  CEPH WORKSHOP - TEST COMPLET
  Date: Mon Jan  5 14:06:47 UTC 2026
==============================================

==============================================
  TP1 - DECOUVERTE DU CLUSTER
==============================================

>>> /scripts/ceph-status.sh

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                       CEPH CLUSTER STATUS                         â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  [0;34mCluster ID:[0m d1756945-144a-49c7-88ed-488b9505157c
[0;36mâ•‘[0m  [0;34mHealth:[0m     [0;32mHEALTH_OK[0m
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mServices:[0m
[0;36mâ•‘[0m    mon: 3 daemons, quorum mon1,mon2,mon3 (age 0s)
[0;36mâ•‘[0m    mgr: mon1(active), standbys: mon2, mon3
[0;36mâ•‘[0m    osd: 3 osds: 3 up, 3 in
[0;36mâ•‘[0m    mds: cephfs:1 {0=mds1=up:active}
[0;36mâ•‘[0m    rgw: 1 daemon active (rgw1)
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mData:[0m
[0;36mâ•‘[0m    pools:   0 pools
[0;36mâ•‘[0m    objects: 0 objects, 0 B
[0;36mâ•‘[0m    usage:   0 GiB used, 3.0 GiB / 3.0 GiB avail
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mI/O:[0m
[0;36mâ•‘[0m    client:   0 B/s rd, 0 B/s wr, 0 op/s rd, 0 op/s wr
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/ceph-health.sh

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    CEPH HEALTH DETAIL                             â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Status: [0;32mHEALTH_OK[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  [0;32mAll checks passed![0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Checks performed:
[0;36mâ•‘[0m    [[0;32mOK[0m] Monitor quorum: 3/3 monitors in quorum
[0;36mâ•‘[0m    [[0;32mOK[0m] OSD status: 3/3 OSDs up and in
[0;36mâ•‘[0m    [[0;32mOK[0m] PG status: All PGs active+clean
[0;36mâ•‘[0m    [[0;32mOK[0m] Disk usage: Below warning threshold
[0;36mâ•‘[0m    [[0;32mOK[0m] Scrub status: No issues found
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mComponent Status:[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m    Monitors:  [0;32m3/3 healthy[0m
[0;36mâ•‘[0m    OSDs:      [0;32m3/3 up, 3/3 in[0m
[0;36mâ•‘[0m    MDS:       [0;32m1 active[0m
[0;36mâ•‘[0m    RGW:       [0;32m1 active[0m
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/ceph-status.sh mon

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                       MONITOR STATUS                              â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Quorum: mon1, mon2, mon3
[0;36mâ•‘[0m
[0;36mâ•‘[0m  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
[0;36mâ•‘[0m  â”‚ Monitor  â”‚ Address       â”‚ Rank   â”‚ Status  â”‚
[0;36mâ•‘[0m  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
[0;36mâ•‘[0m  â”‚ mon1     â”‚ 172.20.0.11    â”‚ 1      â”‚ [0;32mup[0m     â”‚
[0;36mâ•‘[0m  â”‚ mon2     â”‚ 172.20.0.12    â”‚ 2      â”‚ [0;32mup[0m     â”‚
[0;36mâ•‘[0m  â”‚ mon3     â”‚ 172.20.0.13    â”‚ 3      â”‚ [0;32mup[0m     â”‚
[0;36mâ•‘[0m  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/ceph-status.sh osd

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                         OSD TREE                                  â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  ID   CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
[0;36mâ•‘[0m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0;36mâ•‘[0m   -1         3.00000  root default
[0;36mâ•‘[0m   -3         1.00000      host osd1
[0;36mâ•‘[0m    0   hdd   1.00000          osd.0     [0;32mup[0m     1.00000   1.00000
[0;36mâ•‘[0m   -4         1.00000      host osd2
[0;36mâ•‘[0m    1   hdd   1.00000          osd.1     [0;32mup[0m     1.00000   1.00000
[0;36mâ•‘[0m   -5         1.00000      host osd3
[0;36mâ•‘[0m    2   hdd   1.00000          osd.2     [0;32mup[0m     1.00000   1.00000
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


==============================================
  TP2 - ARCHITECTURE CRUSH
==============================================

>>> /scripts/ceph-status.sh crush

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                       CEPH CLUSTER STATUS                         â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  [0;34mCluster ID:[0m d1756945-144a-49c7-88ed-488b9505157c
[0;36mâ•‘[0m  [0;34mHealth:[0m     [0;32mHEALTH_OK[0m
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mServices:[0m
[0;36mâ•‘[0m    mon: 3 daemons, quorum mon1,mon2,mon3 (age 0s)
[0;36mâ•‘[0m    mgr: mon1(active), standbys: mon2, mon3
[0;36mâ•‘[0m    osd: 3 osds: 3 up, 3 in
[0;36mâ•‘[0m    mds: cephfs:1 {0=mds1=up:active}
[0;36mâ•‘[0m    rgw: 1 daemon active (rgw1)
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mData:[0m
[0;36mâ•‘[0m    pools:   0 pools
[0;36mâ•‘[0m    objects: 0 objects, 0 B
[0;36mâ•‘[0m    usage:   0 GiB used, 3.0 GiB / 3.0 GiB avail
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mI/O:[0m
[0;36mâ•‘[0m    client:   0 B/s rd, 0 B/s wr, 0 op/s rd, 0 op/s wr
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


==============================================
  TP3 - GESTION DES POOLS
==============================================

>>> /scripts/pool-create.sh list

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                         POOL LIST                                 â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  NAME                 ID    PGS    SIZE   APP
[0;36mâ•‘[0m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0;36mâ•‘[0m  (aucun pool cree)
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/pool-create.sh mypool 64 3

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘                    POOL CREATED SUCCESSFULLY                      â•‘[0m
[0;32mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;32mâ•‘[0m
[0;32mâ•‘[0m  Pool Name:    mypool
[0;32mâ•‘[0m  Pool ID:      2
[0;32mâ•‘[0m  PG Count:     64
[0;32mâ•‘[0m  Size:         3 (replicas)
[0;32mâ•‘[0m  Min Size:     2
[0;32mâ•‘[0m  Type:         replicated
[0;32mâ•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

[1;33mPour activer une application sur ce pool:[0m
  /scripts/pool-create.sh set mypool application rbd
  /scripts/pool-create.sh set mypool application cephfs
  /scripts/pool-create.sh set mypool application rgw

>>> /scripts/pool-create.sh list

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                         POOL LIST                                 â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  NAME                 ID    PGS    SIZE   APP
[0;36mâ•‘[0m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0;36mâ•‘[0m  mypool               2     64     3      -
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/pool-create.sh stats mypool

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    POOL STATISTICS: mypool[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Configuration:
[0;36mâ•‘[0m    id:             2
[0;36mâ•‘[0m    pg_num:         64
[0;36mâ•‘[0m    pgp_num:        64
[0;36mâ•‘[0m    size:           3
[0;36mâ•‘[0m    min_size:       2
[0;36mâ•‘[0m    type:           replicated
[0;36mâ•‘[0m    application:    
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Statistics:
[0;36mâ•‘[0m    Objects:        0
[0;36mâ•‘[0m    Size:           0 B
[0;36mâ•‘[0m    Reads:          0 B/s
[0;36mâ•‘[0m    Writes:         0 B/s
[0;36mâ•‘[0m    Read Ops:       0 op/s
[0;36mâ•‘[0m    Write Ops:      0 op/s
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


==============================================
  TP4 - RBD BLOCK STORAGE
==============================================

>>> /scripts/pool-create.sh rbd-pool 64 3

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘                    POOL CREATED SUCCESSFULLY                      â•‘[0m
[0;32mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;32mâ•‘[0m
[0;32mâ•‘[0m  Pool Name:    rbd-pool
[0;32mâ•‘[0m  Pool ID:      3
[0;32mâ•‘[0m  PG Count:     64
[0;32mâ•‘[0m  Size:         3 (replicas)
[0;32mâ•‘[0m  Min Size:     2
[0;32mâ•‘[0m  Type:         replicated
[0;32mâ•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

[1;33mPour activer une application sur ce pool:[0m
  /scripts/pool-create.sh set rbd-pool application rbd
  /scripts/pool-create.sh set rbd-pool application cephfs
  /scripts/pool-create.sh set rbd-pool application rgw

>>> /scripts/rbd-manage.sh create rbd-pool/myimage 10G
[0;31mErreur: Usage: /scripts/rbd-manage.sh create <pool/image> --size <size>[0m

>>> /scripts/rbd-manage.sh list rbd-pool

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    RBD IMAGES IN POOL: rbd-pool[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  (aucune image)
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/rbd-manage.sh info rbd-pool/myimage
[0;31mErreur: Image 'rbd-pool/myimage' n'existe pas[0m

>>> /scripts/rbd-manage.sh snap rbd-pool/myimage snap1
[0;31mCommande snap inconnue: rbd-pool/myimage[0m

>>> /scripts/rbd-manage.sh snap-list rbd-pool/myimage
[0;31mCommande inconnue: snap-list[0m

Usage: /scripts/rbd-manage.sh <command> [options]

Commands:
    create <pool/image> --size <size>   Creer une image RBD
    list <pool>                         Lister les images d'un pool
    info <pool/image>                   Informations sur une image
    resize <pool/image> --size <size>   Redimensionner une image
    delete <pool/image>                 Supprimer une image
    map <pool/image>                    Mapper l'image sur un device
    unmap <device>                      Demapper un device
    showmapped                          Afficher les images mappees
    snap create <pool/image@snap>       Creer un snapshot
    snap list <pool/image>              Lister les snapshots
    snap rollback <pool/image@snap>     Rollback vers un snapshot
    snap delete <pool/image@snap>       Supprimer un snapshot

Examples:
    /scripts/rbd-manage.sh create rbd-pool/disk1 --size 10G
    /scripts/rbd-manage.sh map rbd-pool/disk1
    /scripts/rbd-manage.sh snap create rbd-pool/disk1@backup


==============================================
  TP5 - CEPHFS
==============================================

>>> /scripts/cephfs-setup.sh create cephfs

[0;36mCreation du filesystem CephFS: cephfs[0m

Creation du pool de metadonnees...

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘                    POOL CREATED SUCCESSFULLY                      â•‘[0m
[0;32mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;32mâ•‘[0m
[0;32mâ•‘[0m  Pool Name:    cephfs-metadata
[0;32mâ•‘[0m  Pool ID:      4
[0;32mâ•‘[0m  PG Count:     32
[0;32mâ•‘[0m  Size:         3 (replicas)
[0;32mâ•‘[0m  Min Size:     2
[0;32mâ•‘[0m  Type:         replicated
[0;32mâ•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

[1;33mPour activer une application sur ce pool:[0m
  /scripts/pool-create.sh set cephfs-metadata application rbd
  /scripts/pool-create.sh set cephfs-metadata application cephfs
  /scripts/pool-create.sh set cephfs-metadata application rgw
[0;32mPool 'cephfs-metadata': application = cephfs[0m
Creation du pool de donnees...

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘                    POOL CREATED SUCCESSFULLY                      â•‘[0m
[0;32mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;32mâ•‘[0m
[0;32mâ•‘[0m  Pool Name:    cephfs-data
[0;32mâ•‘[0m  Pool ID:      5
[0;32mâ•‘[0m  PG Count:     64
[0;32mâ•‘[0m  Size:         3 (replicas)
[0;32mâ•‘[0m  Min Size:     2
[0;32mâ•‘[0m  Type:         replicated
[0;32mâ•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

[1;33mPour activer une application sur ce pool:[0m
  /scripts/pool-create.sh set cephfs-data application rbd
  /scripts/pool-create.sh set cephfs-data application cephfs
  /scripts/pool-create.sh set cephfs-data application rgw
[0;32mPool 'cephfs-data': application = cephfs[0m

[0;32mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;32mâ•‘                    CEPHFS CREATED                                 â•‘[0m
[0;32mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;32mâ•‘[0m
[0;32mâ•‘[0m  Filesystem:     cephfs
[0;32mâ•‘[0m  Metadata Pool:  cephfs-metadata
[0;32mâ•‘[0m  Data Pool:      cephfs-data
[0;32mâ•‘[0m  MDS:            mds1 (active)
[0;32mâ•‘[0m
[0;32mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

Pour monter le filesystem:
  /scripts/cephfs-setup.sh mount cephfs /mnt/cephfs


>>> /scripts/cephfs-setup.sh list

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                       CEPHFS LIST                                 â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  NAME              METADATA POOL         DATA POOL
[0;36mâ•‘[0m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0;36mâ•‘[0m  cephfs            cephfs-metadata       cephfs-metadata
cephfs-data
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/cephfs-setup.sh status cephfs

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    CEPHFS STATUS: cephfs[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Filesystem: cephfs
[0;36mâ•‘[0m  Status: [0;32mactive[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  MDS:
[0;36mâ•‘[0m    mds1: [0;32mactive[0m (rank 0)
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Pools:
[0;36mâ•‘[0m    Metadata: cephfs-metadata
[0;36mâ•‘[0m    Data:     cephfs-metadata
cephfs-data
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/cephfs-setup.sh mount cephfs /mnt/cephfs

[0;32mCephFS 'cephfs' monte sur /mnt/cephfs[0m

Verification:
  ls /mnt/cephfs

Pour demonter:
  /scripts/cephfs-setup.sh umount /mnt/cephfs


==============================================
  TP6 - OBJECT STORAGE (RGW)
==============================================

>>> /scripts/rgw-setup.sh user create myuser
[0;31mErreur: --uid et --display-name requis[0m

>>> /scripts/rgw-setup.sh user list

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                        RGW USERS                                  â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  (aucun utilisateur)
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/rgw-setup.sh bucket create mybucket myuser
[0;31mCommande bucket inconnue: create[0m

>>> /scripts/rgw-setup.sh bucket list

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                        RGW BUCKETS                                â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  BUCKET                  OWNER        SIZE      OBJECTS
[0;36mâ•‘[0m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0;36mâ•‘[0m  (aucun bucket)
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


==============================================
  TP7 - HAUTE DISPONIBILITE
==============================================

>>> /scripts/simulate-failure.sh status
[0;31mType de panne inconnu: status[0m

Usage: /scripts/simulate-failure.sh <failure_type> [options]

Failure Types:
    osd.<id>            Simuler la panne d'un OSD (0, 1, ou 2)
    mon.<name>          Simuler la panne d'un monitor
    network <duration>  Simuler une panne reseau
    disk-full           Simuler un disque plein
    slow-osd <id>       Simuler un OSD lent
    random              Panne aleatoire
    restore             Restaurer l'etat normal

Examples:
    /scripts/simulate-failure.sh osd.1             # OSD 1 down
    /scripts/simulate-failure.sh mon.mon2          # Monitor 2 down
    /scripts/simulate-failure.sh network 30s       # Panne reseau 30 secondes
    /scripts/simulate-failure.sh restore           # Retour a la normale

Scenarios PCA:
    /scripts/simulate-failure.sh scenario-1        # Perte d'un rack (1 MON + 1 OSD)
    /scripts/simulate-failure.sh scenario-2        # Perte de 2 OSDs
    /scripts/simulate-failure.sh scenario-quorum   # Perte du quorum (2 MONs)


>>> /scripts/simulate-failure.sh osd 2
[0;31mType de panne inconnu: osd[0m

Usage: /scripts/simulate-failure.sh <failure_type> [options]

Failure Types:
    osd.<id>            Simuler la panne d'un OSD (0, 1, ou 2)
    mon.<name>          Simuler la panne d'un monitor
    network <duration>  Simuler une panne reseau
    disk-full           Simuler un disque plein
    slow-osd <id>       Simuler un OSD lent
    random              Panne aleatoire
    restore             Restaurer l'etat normal

Examples:
    /scripts/simulate-failure.sh osd.1             # OSD 1 down
    /scripts/simulate-failure.sh mon.mon2          # Monitor 2 down
    /scripts/simulate-failure.sh network 30s       # Panne reseau 30 secondes
    /scripts/simulate-failure.sh restore           # Retour a la normale

Scenarios PCA:
    /scripts/simulate-failure.sh scenario-1        # Perte d'un rack (1 MON + 1 OSD)
    /scripts/simulate-failure.sh scenario-2        # Perte de 2 OSDs
    /scripts/simulate-failure.sh scenario-quorum   # Perte du quorum (2 MONs)


>>> /scripts/ceph-health.sh

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    CEPH HEALTH DETAIL                             â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Status: [0;32mHEALTH_OK[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  [0;32mAll checks passed![0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Checks performed:
[0;36mâ•‘[0m    [[0;32mOK[0m] Monitor quorum: 3/3 monitors in quorum
[0;36mâ•‘[0m    [[0;32mOK[0m] OSD status: 3/3 OSDs up and in
[0;36mâ•‘[0m    [[0;32mOK[0m] PG status: All PGs active+clean
[0;36mâ•‘[0m    [[0;32mOK[0m] Disk usage: Below warning threshold
[0;36mâ•‘[0m    [[0;32mOK[0m] Scrub status: No issues found
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mComponent Status:[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m    Monitors:  [0;32m3/3 healthy[0m
[0;36mâ•‘[0m    OSDs:      [0;32m3/3 up, 3/3 in[0m
[0;36mâ•‘[0m    MDS:       [0;32m1 active[0m
[0;36mâ•‘[0m    RGW:       [0;32m1 active[0m
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/recovery-check.sh status

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    RECOVERY STATUS                                â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Status: [1;33mRECOVERY IN PROGRESS[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  PGs recovering:  1
[0;36mâ•‘[0m  PGs degraded:    1
[0;36mâ•‘[0m  PGs misplaced:   1
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Recovery Stats:
[0;36mâ•‘[0m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0;36mâ•‘[0m  Objects recovered:  1337
[0;36mâ•‘[0m  Bytes recovered:    420 MB
[0;36mâ•‘[0m  Recovery rate:      67 MB/s
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


>>> /scripts/simulate-failure.sh recover osd 2
[0;31mType de panne inconnu: recover[0m

Usage: /scripts/simulate-failure.sh <failure_type> [options]

Failure Types:
    osd.<id>            Simuler la panne d'un OSD (0, 1, ou 2)
    mon.<name>          Simuler la panne d'un monitor
    network <duration>  Simuler une panne reseau
    disk-full           Simuler un disque plein
    slow-osd <id>       Simuler un OSD lent
    random              Panne aleatoire
    restore             Restaurer l'etat normal

Examples:
    /scripts/simulate-failure.sh osd.1             # OSD 1 down
    /scripts/simulate-failure.sh mon.mon2          # Monitor 2 down
    /scripts/simulate-failure.sh network 30s       # Panne reseau 30 secondes
    /scripts/simulate-failure.sh restore           # Retour a la normale

Scenarios PCA:
    /scripts/simulate-failure.sh scenario-1        # Perte d'un rack (1 MON + 1 OSD)
    /scripts/simulate-failure.sh scenario-2        # Perte de 2 OSDs
    /scripts/simulate-failure.sh scenario-quorum   # Perte du quorum (2 MONs)


>>> /scripts/ceph-health.sh

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    CEPH HEALTH DETAIL                             â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Status: [0;32mHEALTH_OK[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  [0;32mAll checks passed![0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Checks performed:
[0;36mâ•‘[0m    [[0;32mOK[0m] Monitor quorum: 3/3 monitors in quorum
[0;36mâ•‘[0m    [[0;32mOK[0m] OSD status: 3/3 OSDs up and in
[0;36mâ•‘[0m    [[0;32mOK[0m] PG status: All PGs active+clean
[0;36mâ•‘[0m    [[0;32mOK[0m] Disk usage: Below warning threshold
[0;36mâ•‘[0m    [[0;32mOK[0m] Scrub status: No issues found
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m  [0;34mComponent Status:[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m    Monitors:  [0;32m3/3 healthy[0m
[0;36mâ•‘[0m    OSDs:      [0;32m3/3 up, 3/3 in[0m
[0;36mâ•‘[0m    MDS:       [0;32m1 active[0m
[0;36mâ•‘[0m    RGW:       [0;32m1 active[0m
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


==============================================
  TP8 - BACKUP ET RECOVERY
==============================================

>>> /scripts/backup-restore.sh backup config

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    CONFIG BACKUP                                  â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Backup ID: config-20260105-140647
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

Backing up configuration files...
  Backing up ceph.conf...
  Backing up crush-map...
  Backing up mon-map...
  Backing up osd-map...
  Backing up auth-keys...

[0;32mConfiguration backup completed![0m
  Location: /var/lib/ceph/backups/config-20260105-140647


>>> /scripts/backup-restore.sh backup pool mypool

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    POOL BACKUP                                    â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Pool:      mypool
[0;36mâ•‘[0m  Backup ID: pool-mypool-20260105-140649
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

Phase 1: Collecting pool metadata...
  Metadata saved

Phase 2: Exporting objects...
  Progress: 20%
  Progress: 40%
  Progress: 60%
  Progress: 80%
  Progress: 100%

Phase 3: Calculating checksums...
  SHA256: 34731b2f85c4f7ec9263a63f8d869f3b13c41f2b469a8e80f3ecc34af6df65a8

[0;32mBackup completed successfully![0m
  Location: /var/lib/ceph/backups/pool-mypool-20260105-140649


>>> /scripts/backup-restore.sh list

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    AVAILABLE BACKUPS                              â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  ID                              TYPE     SIZE      STATUS
[0;36mâ•‘[0m  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[0;36mâ•‘[0m  config-20260105-140647            config   N/AMB     [0;32mcompleted[0m
[0;36mâ•‘[0m  pool-mypool-20260105-140649       pool     N/AMB     [0;32mcompleted[0m
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


==============================================
  TP9 - BENCHMARKING
==============================================

>>> /scripts/benchmark.sh rados --size 4M --count 10

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    RADOS BENCHMARK                                â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Pool:        benchmark-pool
[0;36mâ•‘[0m  Object Size: 4M
[0;36mâ•‘[0m  Count:       10
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

Phase 1: Write benchmark...
  Write IOPS: 1779
  Write Bandwidth: 6 MB/s

Phase 2: Sequential read benchmark...
  Sequential Read IOPS: 2250
  Sequential Read Bandwidth: 8 MB/s

Phase 3: Random read benchmark...
  Random Read IOPS: 1791
  Random Read Bandwidth: 6 MB/s

[0;32mResultats sauvegardes dans /tmp/ceph-benchmarks/rados-benchmark.json[0m


>>> /scripts/benchmark.sh rbd --size 100M

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    RBD BENCHMARK                                  â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Image Size: 100M
[0;36mâ•‘[0m  Test Type:  fio (4k random, 1M sequential)
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m

Test 1: 4K Random Write...
  IOPS: 10378
  Latency: 114us

Test 2: 4K Random Read...
  IOPS: 16412
  Latency: 63us

Test 3: 1M Sequential Write...
  Bandwidth: 473 MB/s

Test 4: 1M Sequential Read...
  Bandwidth: 889 MB/s

[0;32mResultats sauvegardes dans /tmp/ceph-benchmarks/rbd-benchmark.json[0m


>>> /scripts/benchmark.sh report

[0;36mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[0;36mâ•‘                    BENCHMARK REPORT                               â•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  Generated: Mon Jan  5 14:07:04 UTC 2026
[0;36mâ•‘[0m
[0;36mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[0;36mâ•‘[0m
[0;36mâ•‘[0m  [0;32mRADOS Results:[0m
[0;36mâ•‘[0m    {
[0;36mâ•‘[0m    "timestamp": "2026-01-05T14:07:00+00:00",
[0;36mâ•‘[0m    "type": "rados",
[0;36mâ•‘[0m    "pool": "benchmark-pool",
[0;36mâ•‘[0m    "object_size": "4M",
[0;36mâ•‘[0m    "count": 10,
[0;36mâ•‘[0m    "results": {
[0;36mâ•‘[0m    "write": {
[0;36mâ•‘[0m    "iops": 1779,
[0;36mâ•‘[0m    "bandwidth_mb": 6
[0;36mâ•‘[0m    },
[0;36mâ•‘[0m    "sequential_read": {
[0;36mâ•‘[0m    "iops": 2250,
[0;36mâ•‘[0m    "bandwidth_mb": 8
[0;36mâ•‘[0m    },
[0;36mâ•‘[0m    "random_read": {
[0;36mâ•‘[0m    "iops": 1791,
[0;36mâ•‘[0m    "bandwidth_mb": 6
[0;36mâ•‘[0m    }
[0;36mâ•‘[0m    }
[0;36mâ•‘[0m    }
[0;36mâ•‘[0m
[0;36mâ•‘[0m  [0;32mRBD Results:[0m
[0;36mâ•‘[0m    {
[0;36mâ•‘[0m    "timestamp": "2026-01-05T14:07:04+00:00",
[0;36mâ•‘[0m    "type": "rbd",
[0;36mâ•‘[0m    "image_size": "100M",
[0;36mâ•‘[0m    "results": {
[0;36mâ•‘[0m    "4k_random_write": {
[0;36mâ•‘[0m    "iops": 10378
[0;36mâ•‘[0m    },
[0;36mâ•‘[0m    "4k_random_read": {
[0;36mâ•‘[0m    "iops": 16412
[0;36mâ•‘[0m    },
[0;36mâ•‘[0m    "1m_sequential_write": {
[0;36mâ•‘[0m    "bandwidth_mb": 473
[0;36mâ•‘[0m    },
[0;36mâ•‘[0m    "1m_sequential_read": {
[0;36mâ•‘[0m    "bandwidth_mb": 889
[0;36mâ•‘[0m    }
[0;36mâ•‘[0m    }
[0;36mâ•‘[0m    }
[0;36mâ•‘[0m
[0;36mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


==============================================
  FIN DES TESTS
==============================================
